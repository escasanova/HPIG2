{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#instalacion pip de dependencias necesarias\n",
        "!pip install pyodbc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEE_2UKv5iyn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734041060512,
          "user_tz": 180,
          "elapsed": 4167,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "aa8fa7b7-ef22-41e1-c666-c70c3c4b0f2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading pyodbc-5.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/336.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.0/336.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#carga de datos desde MSSQL y guardado en Storage\n",
        "import pyodbc\n",
        "from google.cloud import storage\n",
        "import csv\n",
        "\n",
        "# Configuración de conexión MSSQL\n",
        "conn = pyodbc.connect(\n",
        "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
        "    'SERVER=192.168.1.41;'\n",
        "    'DATABASE=InsightHub;'\n",
        "    'UID=grupo2;'\n",
        "    'PWD=henry'\n",
        ")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Consulta y exportación a CSV\n",
        "query = \"SELECT * FROM mi_tabla\"\n",
        "cursor.execute(query)\n",
        "\n",
        "csv_file = \"/home/gaz/Descargas/InsightHub.csv\"\n",
        "with open(csv_file, \"w\", newline='', encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([col[0] for col in cursor.description])  # Nombres de columnas\n",
        "    writer.writerows(cursor.fetchall())\n",
        "\n",
        "# Subir a Cloud Storage\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(\"insight_hub\")\n",
        "blob = bucket.blob(\"InsightHub\")\n",
        "blob.upload_from_filename(csv_file)\n",
        "\n",
        "print(\"Archivo subido a Cloud Storage: datos.csv\")\n"
      ],
      "metadata": {
        "id": "lKyNCCVw5Y76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#carga de datos a BigQuery\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Configuración de cliente BigQuery\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Configuración de la tabla de destino\n",
        "table_id = \"starry-aegis-443515-b9.insight_hub.dbo_Compras\"\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        ")\n",
        "\n",
        "# Cargar desde Cloud Storage\n",
        "uri = \"gs://insight_hub/InsightHub.csv\"\n",
        "load_job = client.load_table_from_uri(uri, table_id, job_config)\n",
        "\n",
        "# Esperar la finalización de la carga\n",
        "load_job.result()\n",
        "print(f\"Datos cargados a {table_id}\")\n"
      ],
      "metadata": {
        "id": "xvJ9JXIs6raI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La transformacion de datos se hace a través de queries SQL en BigQuery"
      ],
      "metadata": {
        "id": "83V2jaQX7yaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatizacion de carga\n",
        "import google.cloud.bigquery as bigquery\n",
        "\n",
        "def cargar_a_bigquery(data, context):\n",
        "    # Configuración de cliente BigQuery\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    # Configuración de la tabla de destino\n",
        "    table_id = \"starry-aegis-443515-b9.insight_hub.dbo_Compras\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        autodetect=True,\n",
        "    )\n",
        "\n",
        "    # URI del archivo que activó la función\n",
        "    bucket_name = data[\"bucket\"]\n",
        "    file_name = data[\"name\"]\n",
        "    uri = f\"gs://{bucket_name}/{file_name}\"\n",
        "\n",
        "    # Cargar archivo a BigQuery\n",
        "    load_job = client.load_table_from_uri(uri, table_id, job_config)\n",
        "    load_job.result()\n",
        "    print(f\"Archivo {file_name} cargado a {table_id}\")\n"
      ],
      "metadata": {
        "id": "xUvzaSsb7Li5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}